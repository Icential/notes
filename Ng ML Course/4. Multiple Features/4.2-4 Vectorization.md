Vectorization makes coding faster and easier when computing multiple variable models.

Given $\vec{x},\vec{w},b$ from a model, vectorization basically implements the dot product $\vec{x}\cdot\vec{w}$ into the code to fasten the code

```python
# Initialize vectors in python
x = np.array([x1, x2, x3])
w = np.array([w1, w2, w3])
```

Without vectorization (long and tedious to write):
```python
f = w[0] * x[0] + w[1] * x[1] + w[2] * x[2] + b
```

Still without vectorization but slightly more optimal (still long to process)
```python
f = 0
for i in range(n) # n is length of x or w
	f = f + w[i] * x[i]
f = f + b
```

With vectorization and dot product (most optimal)
```python
f = np.dot(x, w) + b
```

This is faster because it calculates each element of $\vec{w},\vec{x}$ in parallel in one iteration and adds them up in the next iteration.


Vectorization is also helpful in computing gradient descent.

With this new notation, the cost function can be newly written as $J(\vec{w},b)$

Since all weights have derivatives, then there also exists $\vec{d}$ with equal lengths to $\vec{w},\vec{x}$. Moving on step or updating each $w$ in gradient descent would be implemented as $\vec{w}=\vec{w}-\alpha\vec{d}$

```python
d = np.array([d1, d2, d3])

w = w - 0.1 * d
```

Likewise it will result in the following gradient descent equations: $$w_j=w_j-\alpha\frac{\partial}{\partial w_j}J(\vec{w},b)=w_j-\frac{\alpha}{m}\sum_{i=1}^m(f_{w,b}(x)-y^{(i)})x^{(i)}_j$$
and $$b=b-\alpha\frac{\partial}{\partial b}J(\vec{w},b)$$
Like above, it also is faster because it simultaneously calculated the learning rate with all the elements of $\vec{d}$ and then minus that with all corresponding values in $\vec{w}$.


There is an alternative way to find values for $w,b$ ONLY for linear regression called normal equation.
	Doesn't use iterations at all
	Quite slow though
	Might be used by default in machine learning library
