Regularization modifies the weights of not-that-relevant feature or big exponential features to a small value near 0 to avoid overfitting. Or all features' weights to a small value.

This is done by modifying the cost function such that it takes more notice on weights: $$J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^m(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})+\frac{\lambda}{2m}\sum_{j=1}^nw^2_j$$
1. The first part of the function is the familiar mean squared error as seen in [[2.3-4 Cost Function]]. This fits the data.
2. The second part of the function is called the regularization term with a regularization parameter $\lambda>0$. This keeps $w_j$ small and not overfit.
	Change $\lambda$ to emphasize the regularization term more if $w_j$ is not small enough
	If $\lambda=0$, model will overfit
	If $\lambda$ is extremely big, model will severely underfit. $w_j$ becomes 0 and only leave with $b$ which is a straight line for the model.

Combine these two parts together and there is a good balance of fitting the data whilst not overfitting it at the same time.


For linear regression, with this new cost function, a new gradient descent formula is also made only for $w_j$. $b$ is not affected: $$w_j=w_j-\alpha\frac{\partial}{\partial w_j}J(\vec{w}, b)=w_j-\frac{\alpha}{m}\sum^{m}_{i=1}[(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}]+\frac{\lambda}{m}w_j$$
Notice that the equation can be rearranged to a simpler form: $$w_j=\left(1-\frac{\lambda}{m}\right)w_j-\frac{\alpha}{m}\sum^{m}_{i=1}[(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}]$$
This new coefficient for $w_j$ basically multiplies $w_j$ with a number very close to 1 (e.g. 0.9998) which decreases $w_j$ ever so slightly. However in gradient descent, this runs iteratively and it will decrease $w_j$ significantly after each iteration.


For logistic regression, the gradient descent is also the exact same with no change for $b$! $$w_j=\left(1-\frac{\lambda}{m}\right)w_j-\frac{\alpha}{m}\sum^{m}_{i=1}[(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}]$$
But $f_{\vec{w},b}(\vec{x}^{(i)})$ is defined differently as defined in [[6.1-3 Logistic Regression]]