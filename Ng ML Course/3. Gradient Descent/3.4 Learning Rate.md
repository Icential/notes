As seen before, learning rate $\alpha$ can decide if the gradient descent is efficient or not.

If $\alpha$ is too small, it will take tediously long to get to the minimum of the cost function
If $\alpha$ is too big, it may overshoot or skip over the true local minimum of the cost function. In other words, convergence may not be possible (divergence inbound)

Each consecutive step to the local minimum will take smaller steps (smaller $w$ difference, smaller derivative)

Therefore it is not necessary to make a smaller learning rate value to achieve a true local minimum.