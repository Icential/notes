Gradient descent is used to find automatically the smallest cost function $J(w,b)$ value.

Can be used in any model's cost function.

Process:
1. Start with some arbitrary values $w,b$. Maybe 0 and 0, it's whatever.
2. Keep changing $w,b$ SIMULTANEOUSLY such that it reduces $J$
	1. Gradient or derivative of $J$ is at its biggest
	2. Take step at that direction (going downhill)
	3. Simultaneously update (do not overlap updated $w,b$ values in code)
3. Stop when $J$ reaches a local minimum.

Each step can be mathematically represented as: $$w^{(i+1)}=w^{(i)}-\alpha\frac{d}{dw}J(w,b)$$
and $$b^{(i+1)}=b^{(i)}-\alpha\frac{d}{db}J(w,b)$$
where $\alpha$ is the learning rate (controls how big of a step you go downhill)

Convergence (local minimum reached) happens when the change between $w^{(i)}$ and $w^{(i)}$ or $b^{(i)}$ and $b^{(i+1)}$ is trivial.

Here's gradient descent in action:

![[gradient descent simple.png]]

This is in 2D so it only has one parameter $w$.
- Given some $w$, it calculated the slope $J(w)$.
- If the slope is positive (right of minimum) by the above function the new $w$ will go to the left because it has been subtracted by a positive number
- If the slope is negative (left of minimum) by the above function the new $w$ will go to the right because it has been subtracted by a negative number which is an addition
- How much the new $w$ is increased or decreased depends on the learning rate $\alpha$.

If the starting $w$ is at the minimum already, gradient descent will change nothing because the derivative of $w$ is zero and by the equation, nothing changes.

Each consecutive step to the local minimum will take smaller steps (smaller $w$ difference, smaller derivative)

Using calculus, the derivative of $J$ can be calculated in terms of $f$: $$\frac{\partial}{\partial w}J(w,b)=\frac{1}{m}\sum_{i=1}^m(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}$$
and $$\frac{\partial}{\partial b}J(w,b)=\frac{1}{m}\sum_{i=1}^m(f_{w,b}(x^{(i)})-y^{(i)})$$
Therefore the gradient descent equations can be represented as: $$w^{(i+1)}=w^{(i)}-\frac{\alpha}{m}\sum_{i=1}^m(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}$$
and  $$b^{(i+1)}=b^{(i)}-\frac{\alpha}{m}\sum_{i=1}^m(f_{w,b}(x^{(i)})-y^{(i)})$$
The squared error cost function is really nice because there only exist one minimum or in other words, a global minimum.
	This cost function is 2D or 3D is called a convex shape (bowl-shaped)
	As long as the learning rate is appropriate, gradient descent will always converge to the global minimum.

![[gradient descent in action.png]]

There are types of gradient descent:
- Batch gradient descent: Each step looks at the entire training dataset
- Subset gradient descent: Each step looks at a subset or part of the training dataset