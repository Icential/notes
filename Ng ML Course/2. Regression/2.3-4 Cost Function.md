Given linear regression model $f(x)=wx+b$, parameters are $w$ and $b$ because it shapes the linear function.
- $w$ is the slope of the function (weight)
- $b$ is the y-intercept (bias)

Optimally we want $f(x)$ to be as fit as possible for the dataset (implicitly, cross as many data points as possible)

How to know well $f(x)$ fits the data? Use a cost function

Cost function $J$ computes the error of each iteration of prediction and takes the average of all: $$J(w, b) = \frac{1}{2m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2$$
This is called the squared error cost function (very good for linear regression) where:
- $w,b$ are parameters for the linear regression model $f(x)$
- $m$ is the number of datapoints in the dataset
- $i$ is the iteration of each datapoint. $(i)$ means iteration for each data not exponent
- $f(x^{(i)})=\hat{y}^{(i)}$ which is the predicted value
- $y$ is the true targeted label value

To evaluate cost function, plot parameters $w, b$ in respect to $J(w, b)$ and test for each parameter iteration. Goal is to find parameters that calculate a cost function of 0 or as small as possible.

If we let $b=0$, we only get $J(w)$ which means to plot it we use $w$ as x and $J(w)$ as y which is a 2D graph.

However if we let $b\not=0$, we get the normal $J(w, b)$ which means that we need to plot for each parameter. So it will be a 3D graph with axis representing, $w$, $b$ and $J(w,b)$.

For a model with $n$ parameters, $J$ will be graphed in $(n+1)$ dimensions.
	This is what it means when models are multidimensional (multivariate)

In 3D for $J(w,b)$, the minimum value for $J$ is at the middle contour that are surrounded by larger contours. Contours are ovals in 2D.
	Contours can be seen in 2D by plotting x and y as $w$ and $b$ and $J$ as the value for each $w$ and $b$ plot. For each contour, if any $w,b$ pair maps to it, they will have the same value for $J$.

![[cost function in 3d.png]]