The basis of how a neural network works is through their layers which is a group of activation functions given some input vector and outputs another vector of numbers.

A layer will be denoted with a superscript square bracket with the number of its layer (e.g. layer 1's weights and biases will be $\vec{w}^{[1]},b^{[1]}$)

![[1st layer math.png]]

1. Each layer's neuron has its individual $\vec{w},b$ depending on what feature or pattern it is trying to give a possibility on given input $\vec{x}$ of some numbers as elements.
2. This then outputs an activation value $a=g(\vec{w}\cdot \vec{x}+b$ where $g(z)$ is the activation function where in this case is the sigmoid function which is just some probability.
	The general formula for this for layer $l$ and neuron $j$ would be: $$a_j^{[l]}=g(\vec{w}_j^{[l]}\cdot\vec{a}^{[l-1]}+b^{[l]}_j)$$
3. With all the neurons in the layers and their activation values, this makes an output vector $\vec{a}$ or $\vec{a}^{[1]}$ for the 1st hidden layer.
4. This output vector then is the input vector for the next layer which has its own weights and biases and another sigmoid function to output the ultimate scalar probability $f(\vec{x})$ for a desired label.
5. (Optional) Set a threshold value to accept the output possibility as a 0 or 1

![[output layer math.png]]

Inference is when you do the above process and predict something with new input. You go through all the calculations for all layers and all of its neurons from input to layer 1, 2, 3 to the output layer in a forward manner.

In a neural network architecture, it's typical for the amount of neurons per layer in a forward propagation to decrease until its output layer of 1 neuron. (e.g. 20, 10, 5, 3, 1)