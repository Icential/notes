Neural networks can be vectorized using matrix multiplications and are quite good at it even with very big matrices.

The neural network from [[13.1-2 Implementation with Only NumPy]] can be further optimized with matrix multiplication

1. Make all parameters to be 2D matrices
```python
X = np.array([[200, 17]])
W = np.array([[1, -3, 5], [-2, 4, -6]])
B = np.array([[-1, 1, 2]])
```

2. Implement matrix multiplication in `dense()`
```python
def dense(A_in, W, B):
	Z = np.matmul(A_in, W) + B # matrix multiplication
	A_out = g(Z)
	return A_out # also returns a 2D matrix like np.array([[1, 0, 1]])
```

This works mathematically because of matrix multiplication decomposition:

X can be represented as $$X = \begin{bmatrix}200 & 17 \end{bmatrix}$$
and W as $$W = \begin{bmatrix}1 & -3 & 5 \\ -2 & 4 & -6 \end{bmatrix}$$
These are the multiplied and added with bias $$XW +B= \begin{bmatrix}166 & -532 & 898\end{bmatrix}+\begin{bmatrix}-1 & 1 & 2\end{bmatrix}= \begin{bmatrix}165 & -531 & 900\end{bmatrix}$$
Then each element in this calculated matrix gets inputted into the sigmoid function which results in some very close approximation as below $$g(XW+B) = \begin{bmatrix}1 & 0 & 1\end{bmatrix}$$