Typically learning rate $\alpha$ is 0.01 or 0.001.
But also try arbitrarily values of $\alpha$ of 0.001, 0.01, 0.1, 1, etc.
Perhaps find $\alpha$ too big and too small, then use squeeze theorem to find optimal $\alpha$.

If a learning rate is too large, learning curve might look like a sine or cosine function or always goes up.
	Updated parameter value overshoots minima too much to the left or right which creates a larger value of the cost function.

Might also because code is buggy like wrong equation.

A good learning rate should be small enough such that it always decreases cost function per iteration.