How do we know the gradient descent is going near the minima?

We use a learning curve where the x axis is the number of iterations and the y axis is the cost function $J$.

Ideally after each iteration the curve decreases, smooths down and converges/settles down to a value after a significant amount of iterations.
	The amount of iterations to reach convergences varies a lot

![[learning curve.png]]

If it's not converging to value readjust the gradient descent. Change learning rate perhaps.

An automatic convergence test is used to see if convergence has happened in the curve.
- Given some small value $\varepsilon$ like 0.001
- See if $J$ decreases less than $\varepsilon$, if such, declare convergence
- Output $\vec{w},b$