Depending on the label, a model will scale its features $x$ by using weights $w$
	If $x$ is big range then typically $w$ is small
	If $x$ is small range then typically $w$ is big

Because of this, the features can be plotted with big range feature $x_1$ in x and small range feature $x_2$ in y. 
This results in a plot where the plot is stretched so wide in the x axis and not many in y axis.

Likewise in the contour plot for gradient descent with small weight $w_1$ in x and big weight $w_2$ in y.
The contours are too stretched in the y axis and not that much in the x axis.

![[bad feature scaling.png]]

This is bad because it can cause the gradient descent algorithm to take a long time to go into the minima.

Therefore it is necessary to scale the features to a similar size to prevent an inefficient gradient descent to maximize all x and y axis in the plot.

![[good feature scaling.png]]

To scale features, we do normalization.
There are multiple ways to do normalization:

1. Given a range $k_\text{min}\leq x\leq k_\text{max}$ use the following formula: $$x=\frac{x}{k_\text{max}}$$
2.  way to do normalization is called mean normalization. Given the mean average $\mu$ of feature $x$, it will make it so all values are in $[-1,1]$: $$x=\frac{x-\mu}{k_\text{max}-k_\text{min}}$$
3. One more normalization is called Z-score. Given the standard deviation $\sigma$ and mean average $\mu$ of feature $x$, it makes all values in small range: $$x=\frac{x-\mu}{\sigma}$$
When doing normalization, always make sure that the range of the feature values are:
- Small; typically around $[-1,1]$
- Not big; like $[-100,100]$ is too big, do normalization again
- Not too small; like $[-0.001,0.001]$, need to rescale
- Near 0; like $[98,102]$ is too far away from 0, normalize.