For example, we use a classification case of best selling shirts.
	x = price
	y = top seller or not (boolean)

Using how a neural network works, we input price into a neuron and the neuron will output a probability for it to be a top seller using activation function $a$ which in this case of classification is just the logistic function: $$a=f(x)=\frac{1}{1+e^{-(wx+b)}}$$
![[simple neuron.png]]
With this neuron, we connect those with other neurons which will officially make it an actual neural network.

Preferably more inputs (x) need to be used in the model because more data = better performance.

![[multilayer perceptron.png]]

1. A neural network takes input $\vec{x}$ of lets say 4 inputs or numbers.
2. The inputs are taken into the 1st hidden layer (group of activation functions) which accepts 4 number inputs. This then outputs 3 numbers as a vector $\vec{a}$ called activation.
3. This then goes into the input of the 2nd hidden layer that accepts 3 number inputs and outputs 2 number inputs as another $\vec{a}$
4. Finally this goes into final output layer which only consists of 1 neuron/activation function which outputs the probability for a classification case.

Neural networks are really just glorified simpler models (linear regression and logistic regression).
Each hidden layer's neuron calculates a number which represents something that is more useful or correlated to the performance of the model like feature engineering.
So what neural networks are really just automatic feature engineering models.

What to choose on how many neurons per layers and how many layers in a neural network relates to the neural network architecture.