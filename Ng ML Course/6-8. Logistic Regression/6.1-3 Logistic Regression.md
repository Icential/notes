Linear regression is pretty bad for classification problems. So use logistic regression (only used in classification; not regression).
	It's bad because if there exists a positive outlier. It can tip the linear regression very off and makes the decision boundary bad in accuracy.
	Logistic regression is better because it only predicts 0s and 1s flexibly.

![[2d classification.png]]

Classification only has two values like a boolean (yes or no).
	False = 0 = Negative Class, True = 1 = Positive Class


Logistic regression looks like an s-shaped function that's more flexible for setting thresholds because it's more exponential between negative and positive classes.

The logistic function or sigmoid function $g(z)$ is used to output values of 0 and 1. $$g(z)=\frac{1}{1+e^{-z}}$$
![[sigmoid.png]]

So with this, with some regression function, we can input that function into sigmoid to get a logistic regression instead. So using the linear regression $f_{\vec{w},b}(\vec{x})=\vec{w}\cdot\vec{x}+b$ as the input $z$, we get a logistic regression: $$f_{\vec{w},b}(\vec{x})=\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}$$
This will output a probability that a certain feature has a positive class feature (e.g. given tumor size, there is 0.7 or 70% chance the tumor is malignant). Mathematically, $f_{\vec{w},b}(\vec{x})=P(y=1|\vec{x};\vec{w},b)$


The decision boundary is used to ensure that the logistic regression only outputs 0 or 1. 
This is done by setting a threshold in the sigmoid function to what y value is 1 and whatever below is 0.

For example for threshold = 0.5:
$f_{\vec{w},b}(\vec{x})\geq0.5$
$g(z)\geq0.5$
$z\geq0$
$\vec{w}\cdot\vec{x}+b\geq0$

A decision boundary $z$ is able to split negative and positive class with a function. The decision boundary is actually $z=\vec{w}\cdot\vec{x}+b$ where $z=0$ which is the linear regression line. This is good if the dataset can be divided with a straight line

![[linear decision boundary.png]]

So in the above case, $w_1,w_2=1$ and $b=-3$ which makes $x_1+x_2=3$. 
So if $x_1+x_2<3$ then it is a negative class.
If $x_1+x_2\geq 3$ then it is a positive class

Similarly for non-linear decision boundaries, use polynomial regression. 

![[polynomial decision boundary.png]]

In this case, $x_1^2+x_2^2<1$ is the negative class and $x_1^2+x_2^2\geq1$ is the positive class.

The decision boundary can be as complex as you want. Just shape it with any good polynomial regression function you found with exponential manipulation for each feature.

