Again, aim of [[3.1-3,5-7 Gradient Descent]] is to find optimal $\vec{w},b$ that has minimal cost function.

The cost function for logistic regression is $$J(\vec{w}, b)=-\frac{1}{m}\sum_{i=1}^my^{(i)}\log(f_{\vec{w,b}}(\vec{x}^{(i)}))+(1-y^{(i)})\log(1-f_{\vec{w,b}}(\vec{x}^{(i)}))$$
So we use the derivative again to make $\vec{w},b$ into it's global minimum: $$\begin{align*}w_j&=w_j-\alpha\frac{\partial}{\partial w_j}J(\vec{w}, b)=w_j-\frac{\alpha}{m}\sum^{m}_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)} \\ b&=b-\alpha\frac{\partial}{\partial b}J(\vec{w}, b)=w_j-\frac{\alpha}{m}\sum_{i=1}^m(f_{w,b}(x^{(i)})-y^{(i)})\end{align*}$$
Same equation like linear regression but the derivation is completely different.

Make sure to simultaneously update both equations at the same time.

[[5.1-2 Feature Scaling]] is also good for speeding up gradient descent for logistic regression.