The mean squared error cost function used for linear regression is bad for logistic regression.
	Because if you put logistic function into that cost function, it will make a non-convex function which is bad because it has a lot of local minima (lots of ups and downs and squiggles) and can confuse the gradient descent algorithm into a suboptimal minima.

Instead use the following cost function which is a mean average of the loss function$$J(\vec{w}, b)=\frac{1}{m}\sum_{i=1}^mL(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})$$
where $$L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})=\begin{cases}\text{if}~y^{(i)}=1, & -\log(f_{\vec{w,b}}(\vec{x}^{(i)})) \\ \text{if}~y^{(i)}=0, & -\log(1-f_{\vec{w,b}}(\vec{x}^{(i)}))\end{cases}$$
We use log here instead because it punishes better for values of $f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})$ that are between 0 and 1.

![[log cost function.png]]

As seen in their graphs, it punishes accordingly to its corresponding class
	If the true label = 1 (top graph),
		Predicted label close to 1 will return loss that is close to 0
		Predicted label close to 0 will return loss that is a very big number
	If the true label = 0 (bottom graph),
		Predicted label close to 0 will return loss that is close to 0
		Predicted label close 1 will return loss that is a very big number

Now this is great because it's now a convex function again and that there exists a clear global minima.

This loss function can be simplified into one line: $$L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})=-y^{(i)}\log(f_{\vec{w,b}}(\vec{x}^{(i)}))-(1-y^{(i)})\log(1-f_{\vec{w,b}}(\vec{x}^{(i)}))$$
This works because $y^{(i)}$ can only be either 0 or 1. So plugging in those values into the equation gives off the exact same function earlier.

So in full, the cost function is: $$J(\vec{w}, b)=-\frac{1}{m}\sum_{i=1}^my^{(i)}\log(f_{\vec{w,b}}(\vec{x}^{(i)}))+(1-y^{(i)})\log(1-f_{\vec{w,b}}(\vec{x}^{(i)}))$$